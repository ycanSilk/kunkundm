#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨±èŠ±åŠ¨æ¼«æœ€æ–°æ›´æ–°çˆ¬è™«
çˆ¬å–æ¨±èŠ±åŠ¨æ¼«é¦–é¡µçš„æœ€æ–°æ›´æ–°å†…å®¹
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from typing import List, Dict, Optional
from datetime import datetime

class LatestUpdateCrawler:
    def __init__(self):
        self.base_url = "http://www.iyinghua.com"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'http://www.iyinghua.com',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.latest_updates_selector = "body .area div .img ul li"
        
    def fetch_page(self, url: str) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.RequestException as e:
            print(f"è·å–é¡µé¢å¤±è´¥: {e}")
            return None
    
    def parse_latest_updates(self, html: str) -> List[Dict[str, str]]:
        """è§£ææœ€æ–°æ›´æ–°å†…å®¹"""
        soup = BeautifulSoup(html, 'html.parser')
        updates = []
        
        # æ ¹æ®é€‰æ‹©å™¨æŸ¥æ‰¾æœ€æ–°æ›´æ–°å†…å®¹
        update_items = soup.select(self.latest_updates_selector)
        
        for item in update_items:
            try:
                update_data = {}
                
                # æå–å›¾ç‰‡ä¿¡æ¯
                img_tag = item.find('img')
                if img_tag:
                    update_data['cover_image'] = img_tag.get('src', '')
                    update_data['title'] = img_tag.get('alt', '')
                
                # æå–é“¾æ¥ä¿¡æ¯
                link_tag = item.find('a')
                if link_tag:
                    href = link_tag.get('href', '')
                    if href.startswith('/'):
                        href = self.base_url + href
                    update_data['detail_url'] = href
                    
                    # æå–æ ‡é¢˜ï¼ˆå¦‚æœå›¾ç‰‡ä¸­æ²¡æœ‰ï¼‰
                    if not update_data.get('title'):
                        update_data['title'] = link_tag.get('title', '') or link_tag.text.strip()
                
                # æå–æ›´æ–°ä¿¡æ¯
                info_spans = item.find_all('span')
                for span in info_spans:
                    span_text = span.text.strip()
                    if 'æ›´æ–°è‡³' in span_text:
                        update_data['episode_info'] = span_text
                    elif 'ç±»å‹ï¼š' in span_text:
                        update_data['anime_type'] = span_text.replace('ç±»å‹ï¼š', '').strip()
                
                # æå–é›†æ•°ä¿¡æ¯
                episode_match = re.search(r'æ›´æ–°è‡³(\d+)é›†', str(item))
                if episode_match:
                    update_data['current_episode'] = int(episode_match.group(1))
                
                if update_data.get('title') and update_data.get('detail_url'):
                    updates.append(update_data)
                    
            except Exception as e:
                print(f"è§£æé¡¹ç›®å¤±è´¥: {e}")
                continue
        
        return updates
    
    def crawl_latest_updates(self) -> Dict[str, any]:
        """çˆ¬å–æœ€æ–°æ›´æ–°"""
        print("å¼€å§‹çˆ¬å–æ¨±èŠ±åŠ¨æ¼«æœ€æ–°æ›´æ–°...")
        
        html = self.fetch_page(self.base_url)
        if not html:
            return {
                'success': False,
                'error': 'æ— æ³•è·å–é¡µé¢å†…å®¹',
                'data': [],
                'timestamp': datetime.now().isoformat()
            }
        
        updates = self.parse_latest_updates(html)
        
        result = {
            'success': True,
            'total_count': len(updates),
            'data': updates,
            'timestamp': datetime.now().isoformat(),
            'source_url': self.base_url
        }
        
        return result
    
    def save_to_json(self, data: Dict[str, any], filename: str = None) -> str:
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'latest_updates_{timestamp}.json'
        
        filepath = filename
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
            return filepath
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def print_summary(self, data: Dict[str, any]) -> None:
        """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
        if data['success']:
            print(f"\nâœ… çˆ¬å–æˆåŠŸ!")
            print(f"ğŸ“Š æ€»æ›´æ–°æ•°é‡: {data['total_count']}")
            print(f"â° çˆ¬å–æ—¶é—´: {data['timestamp']}")
            
            if data['data']:
                print("\nğŸ“‹ æœ€æ–°æ›´æ–°åˆ—è¡¨:")
                for i, item in enumerate(data['data'][:5], 1):
                    print(f"{i}. {item.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                    if 'episode_info' in item:
                        print(f"   ğŸ“º {item['episode_info']}")
                    if 'anime_type' in item:
                        print(f"   ğŸ·ï¸ ç±»å‹: {item['anime_type']}")
                    print(f"   ğŸ”— {item.get('detail_url', '')}")
                    print()
        else:
            print(f"âŒ çˆ¬å–å¤±è´¥: {data.get('error', 'æœªçŸ¥é”™è¯¯')}")

def main():
    """ä¸»å‡½æ•°"""
    crawler = LatestUpdateCrawler()
    
    try:
        # çˆ¬å–æœ€æ–°æ›´æ–°
        result = crawler.crawl_latest_updates()
        
        # æ‰“å°æ‘˜è¦
        crawler.print_summary(result)
        
        # ä¿å­˜æ•°æ®
        if result['success']:
            crawler.save_to_json(result)
        
        return result
        
    except Exception as e:
        print(f"çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}")
        return {
            'success': False,
            'error': str(e),
            'data': [],
            'timestamp': datetime.now().isoformat()
        }

if __name__ == "__main__":
    main()