#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨±èŠ±åŠ¨æ¼«åˆ†é›†URLçˆ¬è™« - å‘½ä»¤è¡Œç‰ˆæœ¬
æ”¯æŒæ‰¹é‡çˆ¬å–å’Œå¤šç§è¾“å‡ºæ ¼å¼
"""

import requests
from bs4 import BeautifulSoup
import re
import json
import argparse
import sys
import os
from urllib.parse import urljoin, urlparse
from datetime import datetime

class AnimeEpisodeCrawler:
    def __init__(self, base_url="http://www.iyinghua.com"):
        self.base_url = base_url
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        
    def validate_url(self, url):
        """éªŒè¯URLæ ¼å¼"""
        try:
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                return False
            if 'iyinghua.com' not in parsed.netloc:
                return False
            if not parsed.path.startswith('/show/'):
                return False
            return True
        except:
            return False
            
    def extract_anime_info(self, soup):
        """æå–åŠ¨æ¼«åŸºæœ¬ä¿¡æ¯"""
        title = "æœªçŸ¥åŠ¨æ¼«"
        try:
            # å°è¯•å¤šç§æ–¹å¼æå–æ ‡é¢˜
            title_elem = soup.find('h1')
            if title_elem:
                title = title_elem.get_text().strip()
            else:
                title_elem = soup.find('title')
                if title_elem:
                    title = title_elem.get_text().strip()
                    title = re.sub(r'[|_-].*', '', title).strip()
        except:
            pass
        return title
        
    def extract_episodes(self, soup):
        """æå–åˆ†é›†ä¿¡æ¯"""
        episodes = []
        
        # ç­–ç•¥1: æŸ¥æ‰¾æ ‡å‡†åˆ†é›†åˆ—è¡¨
        movurl_div = soup.find('div', class_='movurl')
        if movurl_div:
            links = movurl_div.find_all('a', href=True)
        else:
            # ç­–ç•¥2: å¹¿æ³›æœç´¢æ‰€æœ‰å¯èƒ½çš„é“¾æ¥
            links = soup.find_all('a', href=re.compile(r'/v/\d+-\d+\.html'))
            
        for link in links:
            href = link.get('href', '').strip()
            title = link.get_text().strip()
            
            # éªŒè¯é“¾æ¥æ ¼å¼
            if href and re.match(r'/v/\d+-\d+\.html', href):
                full_url = urljoin(self.base_url, href)
                
                # æå–é›†æ•°
                episode_match = re.search(r'-(\d+)\.html', href)
                episode_num = int(episode_match.group(1)) if episode_match else 0
                
                episodes.append({
                    'episode': episode_num,
                    'title': title,
                    'url': full_url,
                    'relative_url': href
                })
        
        # æŒ‰é›†æ•°æ’åº
        episodes.sort(key=lambda x: x['episode'])
        return episodes
        
    def crawl_single_anime(self, url):
        """çˆ¬å–å•ä¸ªåŠ¨æ¼«çš„åˆ†é›†ä¿¡æ¯"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å–: {url}")
        
        if not self.validate_url(url):
            print("âŒ æ— æ•ˆçš„åŠ¨æ¼«é¡µé¢URL")
            return None
            
        try:
            print("ğŸŒ å‘é€è¯·æ±‚...")
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            print(f"âœ… è·å–é¡µé¢æˆåŠŸ ({len(response.text)} å­—ç¬¦)")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            anime_title = self.extract_anime_info(soup)
            episodes = self.extract_episodes(soup)
            
            if not episodes:
                print("âš ï¸ æœªæ‰¾åˆ°åˆ†é›†ä¿¡æ¯")
                return None
                
            result = {
                "anime_name": anime_title,
                "total_episodes": len(episodes),
                "source_url": url,
                "base_url": self.base_url,
                "crawl_time": datetime.now().isoformat(),
                "episodes": episodes
            }
            
            print(f"âœ… çˆ¬å–å®Œæˆï¼å…±æ‰¾åˆ° {len(episodes)} é›†")
            return result
            
        except requests.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
            return None
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {str(e)}")
            return None
            
    def crawl_batch(self, urls):
        """æ‰¹é‡çˆ¬å–"""
        results = []
        for url in urls:
            result = self.crawl_single_anime(url.strip())
            if result:
                results.append(result)
        return results
        
    def export_data(self, data, filename, format_type='json'):
        """å¯¼å‡ºæ•°æ®"""
        try:
            if format_type == 'json':
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                    
            elif format_type == 'txt':
                if isinstance(data, list):
                    # æ‰¹é‡ç»“æœ
                    with open(filename, 'w', encoding='utf-8') as f:
                        for item in data:
                            f.write(f"ğŸŒ¸ {item['anime_name']}\n")
                            f.write(f"æ€»é›†æ•°: {item['total_episodes']}\n")
                            f.write(f"é¡µé¢URL: {item['source_url']}\n")
                            f.write("=" * 50 + "\n")
                            for ep in item['episodes']:
                                f.write(f"ç¬¬{ep['episode']:02d}é›†: {ep['url']}\n")
                            f.write("\n")
                else:
                    # å•ä¸ªç»“æœ
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(f"ğŸŒ¸ {data['anime_name']}\n")
                        f.write(f"æ€»é›†æ•°: {data['total_episodes']}\n")
                        f.write(f"é¡µé¢URL: {data['source_url']}\n")
                        f.write("=" * 50 + "\n")
                        for ep in data['episodes']:
                            f.write(f"ç¬¬{ep['episode']:02d}é›†: {ep['url']}\n")
                            
            elif format_type == 'csv':
                import csv
                with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                    writer = csv.writer(f)
                    writer.writerow(['åŠ¨æ¼«åç§°', 'é›†æ•°', 'æ ‡é¢˜', 'å®Œæ•´URL', 'ç›¸å¯¹URL'])
                    
                    if isinstance(data, list):
                        for item in data:
                            for ep in item['episodes']:
                                writer.writerow([
                                    item['anime_name'],
                                    ep['episode'],
                                    ep['title'],
                                    ep['url'],
                                    ep['relative_url']
                                ])
                    else:
                        for ep in data['episodes']:
                            writer.writerow([
                                data['anime_name'],
                                ep['episode'],
                                ep['title'],
                                ep['url'],
                                ep['relative_url']
                            ])
                            
            print(f"âœ… æ•°æ®å·²å¯¼å‡ºåˆ°: {filename}")
            return True
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ¨±èŠ±åŠ¨æ¼«åˆ†é›†URLçˆ¬è™«')
    parser.add_argument('url', nargs='?', help='åŠ¨æ¼«è¯¦æƒ…é¡µURL')
    parser.add_argument('-f', '--file', help='ä»æ–‡ä»¶è¯»å–URLåˆ—è¡¨')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('-t', '--format', choices=['json', 'txt', 'csv'], default='json', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--base-url', default='http://www.iyinghua.com', help='åŸºç¡€URL')
    
    args = parser.parse_args()
    
    if not args.url and not args.file:
        print("âŒ è¯·æä¾›URLæˆ–ä½¿ç”¨-få‚æ•°æŒ‡å®šæ–‡ä»¶")
        print("ç¤ºä¾‹:")
        print("  python anime_episode_crawler.py http://www.iyinghua.com/show/6556.html")
        print("  python anime_episode_crawler.py -f urls.txt -o results.json")
        return
        
    crawler = AnimeEpisodeCrawler(args.base_url)
    
    # è·å–URLåˆ—è¡¨
    urls = []
    if args.file:
        try:
            with open(args.file, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {args.file}")
            return
    else:
        urls = [args.url]
        
    # çˆ¬å–æ•°æ®
    if len(urls) == 1:
        data = crawler.crawl_single_anime(urls[0])
    else:
        data = crawler.crawl_batch(urls)
        
    if not data:
        print("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
        return
        
    # å¯¼å‡ºæ•°æ®
    if args.output:
        filename = args.output
    else:
        if len(urls) == 1:
            # å•ä¸ªç»“æœ
            anime_name = data['anime_name'] if isinstance(data, dict) else data[0]['anime_name']
            filename = f"{anime_name}_episodes.{args.format}"
        else:
            # æ‰¹é‡ç»“æœ
            filename = f"batch_episodes.{args.format}"
            
    crawler.export_data(data, filename, args.format)

if __name__ == "__main__":
    main()