# åŠ¨æ¼«çˆ¬è™«ç³»ç»Ÿ

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

è¿™ä¸ªçˆ¬è™«ç³»ç»Ÿä¸“ä¸ºanime-siteé¡¹ç›®è®¾è®¡ï¼Œç”¨äºä»å„å¤§åŠ¨æ¼«ç½‘ç«™çˆ¬å–æ•°æ®å¹¶å‘é€åˆ°Next.jsåç«¯APIã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
crawler/
â”œâ”€â”€ anime_crawler.py      # ä¸»çˆ¬è™«ç±»
â”œâ”€â”€ run_crawler.py        # å¯åŠ¨è„šæœ¬
â”œâ”€â”€ config.py            # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt     # Pythonä¾èµ–
â”œâ”€â”€ .env.example        # ç¯å¢ƒå˜é‡æ¨¡æ¿
â””â”€â”€ README.md           # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# è¿›å…¥çˆ¬è™«ç›®å½•
cd crawler/

# å®‰è£…Pythonä¾èµ–
pip install -r requirements.txt

# å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘é…ç½®æ–‡ä»¶
vim .env
```

### 2. æµ‹è¯•è¿æ¥

```bash
# ç¡®ä¿Next.jsæœåŠ¡æ­£åœ¨è¿è¡Œ
npm run dev

# æµ‹è¯•çˆ¬è™«è¿æ¥
python run_crawler.py --action test
```

### 3. çˆ¬å–æ¨¡æ‹Ÿæ•°æ®

```bash
# çˆ¬å–20æ¡æ¨¡æ‹Ÿæ•°æ®
python run_crawler.py --action crawl --count 20

# æŒ‡å®šAPIåœ°å€
python run_crawler.py --api-url http://localhost:3000 --count 50
```

## ğŸ“‹ APIç«¯ç‚¹

### æ¥æ”¶æ•°æ®
- **POST /api/crawler** - æ¥æ”¶çˆ¬è™«æ•°æ®
- **POST /api/anime** - æ¥æ”¶åŠ¨æ¼«æ•°æ®

### è·å–æ•°æ®
- **GET /api/anime?action=list** - è·å–åŠ¨æ¼«åˆ—è¡¨
- **GET /api/anime?action=weekly** - è·å–æ¯å‘¨æ›´æ–°
- **GET /api/anime?action=search&q=å…³é”®è¯** - æœç´¢åŠ¨æ¼«

## ğŸ”„ æ•°æ®æ ¼å¼

### åŠ¨æ¼«åˆ—è¡¨æ•°æ®
```json
{
  "crawler_type": "anime_list",
  "data": [
    {
      "id": "1",
      "title": "åŠ¨æ¼«åç§°",
      "coverImage": "https://example.com/cover.jpg",
      "episodes": 12,
      "currentEpisode": 8,
      "genre": ["å¥‡å¹»", "å†’é™©"],
      "description": "åŠ¨æ¼«ç®€ä»‹",
      "year": 2024,
      "rating": 8.5
    }
  ]
}
```

### æ¯å‘¨æ›´æ–°æ•°æ®
```json
{
  "crawler_type": "weekly_updates",
  "data": {
    "å‘¨ä¸€": [
      {
        "id": "anime_1",
        "title": "åŠ¨æ¼«åç§°",
        "episode": 9,
        "coverImage": "https://example.com/cover.jpg"
      }
    ]
  }
}
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡ (.env)
```bash
# APIé…ç½®
API_BASE_URL=http://localhost:3000

# æ•°æ®æº
YHDM_BASE_URL=https://www.yhdm.tv
AGEFANS_BASE_URL=https://www.agefans.cc

# çˆ¬è™«å‚æ•°
CRAWLER_DELAY=1.0
MAX_RETRIES=3
REQUEST_TIMEOUT=10
```

### çˆ¬è™«é…ç½® (config.py)
- `REQUEST_DELAY`: è¯·æ±‚é—´éš”æ—¶é—´(ç§’)
- `MAX_RETRIES`: æœ€å¤§é‡è¯•æ¬¡æ•°
- `REQUEST_TIMEOUT`: è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
- `MAX_CONCURRENT`: æœ€å¤§å¹¶å‘æ•°

## ğŸ”§ æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„æ•°æ®æº

1. åœ¨ `config.py` çš„ `DATA_SOURCES` ä¸­æ·»åŠ æ–°é…ç½®
2. åˆ›å»ºå¯¹åº”çš„çˆ¬è™«ç±»ç»§æ‰¿ `AnimeCrawler`
3. åœ¨ `run_crawler.py` ä¸­æ·»åŠ æ–°çš„çˆ¬è™«ç±»å‹

### ç¤ºä¾‹ï¼šåˆ›å»ºçœŸå®çˆ¬è™«

```python
from anime_crawler import AnimeCrawler
import requests
from bs4 import BeautifulSoup

class RealAnimeCrawler(AnimeCrawler):
    def crawl_yhdm_list(self):
        """çˆ¬å–æ¨±èŠ±åŠ¨æ¼«åˆ—è¡¨"""
        url = "https://www.yhdm.tv/list/"
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # è§£ææ•°æ®é€»è¾‘...
        anime_list = []
        # æ·»åŠ åˆ°anime_list...
        
        return self.send_data("anime_list", anime_list, url)
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **è¿æ¥å¤±è´¥**
   - æ£€æŸ¥Next.jsæœåŠ¡æ˜¯å¦è¿è¡Œ
   - ç¡®è®¤APIåœ°å€æ­£ç¡®
   - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®

2. **æ•°æ®æ ¼å¼é”™è¯¯**
   - éªŒè¯JSONæ ¼å¼
   - æ£€æŸ¥å¿…éœ€å­—æ®µ
   - æŸ¥çœ‹APIå“åº”

3. **çˆ¬è™«è¢«å°**
   - å¢åŠ è¯·æ±‚å»¶è¿Ÿ
   - ä½¿ç”¨ä»£ç†IP
   - æ›´æ¢User-Agent

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

- æ—¥å¿—æ–‡ä»¶ï¼š`crawler.log`
- å®æ—¶ç›‘æ§ï¼š`tail -f crawler.log`
- é”™è¯¯ç»Ÿè®¡ï¼šæŸ¥çœ‹æ—¥å¿—ä¸­çš„ERRORçº§åˆ«

## ğŸ“ æ³¨æ„äº‹é¡¹

- è¯·éµå®ˆç›®æ ‡ç½‘ç«™çš„robots.txtè§„åˆ™
- åˆç†è®¾ç½®è¯·æ±‚é—´éš”ï¼Œé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆå‹åŠ›
- ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨æ•°æ®åº“å­˜å‚¨æ•°æ®
- å®šæœŸæ¸…ç†æ—¥å¿—æ–‡ä»¶é¿å…è¿‡å¤§